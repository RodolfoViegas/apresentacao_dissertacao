@article{atiya2020,
  title={Why does forecast combination work so well?},
  author={Atiya, Amir F},
  journal={International Journal of Forecasting},
  volume={36},
  number={1},
  pages={197--200},
  year={2020},
  publisher={Elsevier}
}

@article{BatesGranger1969,
	author = {J. M. Bates and C. W. J. Granger},
	title = {The Combination of Forecasts},
	journal = {Journal of the Operational Research Society},
	volume = {20},
	number = {4},
	pages = {451--468},
	year = {1969},
	publisher = {Taylor \& Francis},
	doi = {10.1057/jors.1969.103},
	URL = {https://doi.org/10.1057/jors.1969.103},
	eprint = {https://doi.org/10.1057/jors.1969.103}
}

@article{wang2006characteristic,
  title={Characteristic-based clustering for time series data},
  author={Wang, Xiaozhe and Smith, Kate and Hyndman, Rob},
  journal={Data mining and knowledge Discovery},
  volume={13},
  pages={335--364},
  year={2006},
  publisher={Springer}
}
@article{wang2023review,
  title={Forecast combinations: An over 50-year review},
  author={Wang, Xiaoqian and Hyndman, Rob J and Li, Feng and Kang, Yanfei},
  journal={International Journal of Forecasting},
  volume={39},
  number={4},
  pages={1518--1547},
  year={2023},
  publisher={Elsevier}
}

@article{stock2004,
  title={Combination forecasts of output growth in a seven-country data set},
  author={Stock, James H and Watson, Mark W},
  journal={Journal of forecasting},
  volume={23},
  number={6},
  pages={405--430},
  year={2004},
  publisher={Wiley Online Library}
}

@article{newbold1974,
  title={Experience with forecasting univariate time series and the combination of forecasts},
  author={Newbold, Paul and Granger, Clive WJ},
  journal={Journal of the Royal Statistical Society: Series A (General)},
  volume={137},
  number={2},
  pages={131--146},
  year={1974},
  publisher={Wiley Online Library},
  doi = {https://doi.org/10.2307/2344546}
}


@article{granger1984,
author = {Granger, Clive W. J. and Ramanathan, Ramu},
title = {Improved methods of combining forecasts},
journal = {Journal of Forecasting},
volume = {3},
number = {2},
pages = {197-204},
keywords = {Combining, ARMA models, Econometrics},
doi = {https://doi.org/10.1002/for.3980030207},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/for.3980030207},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/for.3980030207},
abstract = {Abstract It is well known that a linear combination of forecasts can outperform individual forecasts. The common practice, however, is to obtain a weighted average of forecasts, with the weights adding up to unity. This paper considers three alternative approaches to obtaining linear combinations. It is shown that the best method is to add a constant term and not to constrain the weights to add to unity. These methods are tested with data on forecasts of quarterly hog prices, both within and out of sample. It is demonstrated that the optimum method proposed here is superior to the common practice of letting the weights add up to one.},
year = {1984}
}

@article{conflitti2015,
  title={Optimal combination of survey forecasts},
  author={Conflitti, Cristina and De Mol, Christine and Giannone, Domenico},
  journal={International Journal of Forecasting},
  volume={31},
  number={4},
  pages={1096--1103},
  year={2015},
  publisher={Elsevier}
}

@article{winkler_makridakis_1983,
 ISSN = {00359238, 23972327},
 URL = {http://www.jstor.org/stable/2982011},
 abstract = {Aggregating information by combining forecasts from two or more forecasting methods is an alternative to using just a single method. In this paper we provide extensive empirical results showing that combined forecasts obtained through weighted averages can be quite accurate. Five procedures for estimating weights are investigated, and two appear to be superior to the others. These two procedures provide forecasts that are more accurate overall than forecasts from individual methods. Furthermore, they are superior to forecasts found from a simple unweighted average of the same methods.},
 author = {Robert L. Winkler and Spyros Makridakis},
 journal = {Journal of the Royal Statistical Society. Series A (General)},
 number = {2},
 pages = {150--157},
 publisher = {[Royal Statistical Society, Oxford University Press]},
 title = {The Combination of Forecasts},
 urldate = {2025-05-19},
 volume = {146},
 year = {1983}
}

@article{M1,
author = {Makridakis, S. and Andersen, A. and Carbone, R. and Fildes, R. and Hibon, M. and Lewandowski, R. and Newton, J. and Parzen, E. and Winkler, R.},
title = {The accuracy of extrapolation (time series) methods: Results of a forecasting competition},
journal = {Journal of Forecasting},
volume = {1},
number = {2},
pages = {111-153},
keywords = {Forecasting, Time series, Evaluation, Accuracy, Comparison, Empirical study},
doi = {https://doi.org/10.1002/for.3980010202},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/for.3980010202},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/for.3980010202},
abstract = {Abstract In the last few decades many methods have become available for forecasting. As always, when alternatives exist, choices need to be made so that an appropriate forecasting method can be selected and used for the specific situation being considered. This paper reports the results of a forecasting competition that provides information to facilitate such choice. Seven experts in each of the 24 methods forecasted up to 1001 series for six up to eighteen time horizons. The results of the competition are presented in this paper whose purpose is to provide empirical evidence about differences found to exist among the various extrapolative (time series) methods used in the competition.},
year = {1982}
}

@article{makridakis1983averages,
	title={Averages of forecasts: Some empirical results},
	author={Makridakis, Spyros and Winkler, Robert L},
	journal={Management science},
	volume={29},
	number={9},
	pages={987--996},
	year={1983},
	publisher={Informs}
}

@article{M3,
title = {The M3-Competition: results, conclusions and implications},
journal = {International Journal of Forecasting},
volume = {16},
number = {4},
pages = {451-476},
year = {2000},
note = {The M3- Competition},
issn = {0169-2070},
doi = {https://doi.org/10.1016/S0169-2070(00)00057-1},
url = {https://www.sciencedirect.com/science/article/pii/S0169207000000571},
author = {Spyros Makridakis and Michèle Hibon},
keywords = {Comparative methods — time series: univariate, Forecasting competitions, M-Competition, Forecasting methods, Forecasting accuracy},
abstract = {This paper describes the M3-Competition, the latest of the M-Competitions. It explains the reasons for conducting the competition and summarizes its results and conclusions. In addition, the paper compares such results/conclusions with those of the previous two M-Competitions as well as with those of other major empirical studies. Finally, the implications of these results and conclusions are considered, their consequences for both the theory and practice of forecasting are explored and directions for future research are contemplated.}
}

@article{M5_makridakis,
title = {M5 accuracy competition: Results, findings, and conclusions},
journal = {International Journal of Forecasting},
volume = {38},
number = {4},
pages = {1346-1364},
year = {2022},
note = {Special Issue: M5 competition},
issn = {0169-2070},
doi = {https://doi.org/10.1016/j.ijforecast.2021.11.013},
url = {https://www.sciencedirect.com/science/article/pii/S0169207021001874},
author = {Spyros Makridakis and Evangelos Spiliotis and Vassilios Assimakopoulos},
keywords = {Forecasting competitions, M competitions, Accuracy, Time series, Machine learning, Retail sales forecasting},
abstract = {In this study, we present the results of the M5 “Accuracy” competition, which was the first of two parallel challenges in the latest M competition with the aim of advancing the theory and practice of forecasting. The main objective in the M5 “Accuracy” competition was to accurately predict 42,840 time series representing the hierarchical unit sales for the largest retail company in the world by revenue, Walmart. The competition required the submission of 30,490 point forecasts for the lowest cross-sectional aggregation level of the data, which could then be summed up accordingly to estimate forecasts for the remaining upward levels. We provide details of the implementation of the M5 “Accuracy” challenge, as well as the results and best performing methods, and summarize the major findings and conclusions. Finally, we discuss the implications of these findings and suggest directions for future research.}
}

@article{M5_1,
title = {Simple averaging of direct and recursive forecasts via partial pooling using machine learning},
journal = {International Journal of Forecasting},
volume = {38},
number = {4},
pages = {1386-1399},
year = {2022},
note = {Special Issue: M5 competition},
issn = {0169-2070},
doi = {https://doi.org/10.1016/j.ijforecast.2021.11.007},
url = {https://www.sciencedirect.com/science/article/pii/S0169207021001813},
author = {YeonJun In and Jae-Yoon Jung},
keywords = {Direct and recursive multi-step forecasting, Multi-level data, Forecast averaging, Machine learning, LightGBM},
abstract = {This article introduces the winning method at the M5 Accuracy competition. The presented method takes a simple manner of averaging the results of multiple base forecasting models that have been constructed via partial pooling of multi-level data. All base forecasting models of adopting direct or recursive multi-step forecasting methods are trained by the machine learning technique, LightGBM, from three different levels of data pools. At the competition, the simple averaging of the multiple direct and recursive forecasting models, called DRFAM, obtained the complementary effects between direct and recursive multi-step forecasting of the multi-level product sales to improve the accuracy and the robustness.}
}

@article{kolassa2011,
  title={Combining exponential smoothing forecasts using Akaike weights},
  author={Kolassa, Stephan},
  journal={International Journal of Forecasting},
  volume={27},
  number={2},
  pages={238--251},
  year={2011},
  publisher={Elsevier}
}


@article{bergmeir2016,
  title={Bagging exponential smoothing methods using STL decomposition and Box--Cox transformation},
  author={Bergmeir, Christoph and Hyndman, Rob J and Ben{\'\i}tez, Jos{\'e} M},
  journal={International journal of forecasting},
  volume={32},
  number={2},
  pages={303--312},
  year={2016},
  publisher={Elsevier}
}

@article{petropoulos2018bagging,
  title={Exploring the sources of uncertainty: Why does bagging for time series forecasting work?},
  author={Petropoulos, Fotios and Hyndman, Rob J and Bergmeir, Christoph},
  journal={European Journal of Operational Research},
  volume={268},
  number={2},
  pages={545--554},
  year={2018},
  publisher={Elsevier}
}

@article{fforma,
  title={FFORMA: Feature-based forecast model averaging},
  author={Montero-Manso, Pablo and Athanasopoulos, George and Hyndman, Rob J and Talagala, Thiyanga S},
  journal={International Journal of Forecasting},
  volume={36},
  number={1},
  pages={86--92},
  year={2020},
  publisher={Elsevier},
  doi = {https://doi.org/10.1016/j.ijforecast.2019.02.011}
}

@article{kourentzes2014mode,
	title={Neural network ensemble operators for time series forecasting},
	author={Kourentzes, Nikolaos and Barrow, Devon K and Crone, Sven F},
	journal={Expert Systems with Applications},
	volume={41},
	number={9},
	pages={4235--4244},
	year={2014},
	publisher={Elsevier}
}

@article{petropoulos2020,
	title={A simple combination of univariate models},
	author={Petropoulos, Fotios and Svetunkov, Ivan},
	journal={International journal of forecasting},
	volume={36},
	number={1},
	pages={110--115},
	year={2020},
	publisher={Elsevier}
}

@article{Makridakis2023,
        author = {Spyros Makridakis and Evangelos Spiliotis and Vassilios Assimakopoulos and Artemios-Anargyros Semenoglou and Gary Mulder and Konstantinos Nikolopoulos and},
        title = {Statistical, machine learning and deep learning forecasting methods: Comparisons and ways forward},
        journal = {Journal of the Operational Research Society},
        volume = {74},
        number = {3},
        pages = {840--859},
        year = {2023},
        publisher = {Taylor \& Francis},
        doi = {10.1080/01605682.2022.2118629},
        URL = { https://doi.org/10.1080/01605682.2022.2118629},
        eprint = {https://doi.org/10.1080/01605682.2022.2118629}
}

@article{wolpert1992,
  title={Stacked generalization},
  author={Wolpert, David H},
  journal={Neural networks},
  volume={5},
  number={2},
  pages={241--259},
  year={1992},
  publisher={Elsevier}
}

@article{ribeiro_coelho2020,
	title={Ensemble approach based on bagging, boosting and stacking for short-term prediction in agribusiness time series},
	author={Ribeiro, Matheus Henrique Dal Molin and dos Santos Coelho, Leandro},
	journal={Applied soft computing},
	volume={86},
	pages={105837},
	year={2020},
	publisher={Elsevier}
}

@article{RIBEIRO2019,
title = {Enhanced ensemble structures using wavelet neural networks applied to short-term load forecasting},
journal = {Engineering Applications of Artificial Intelligence},
volume = {82},
pages = {272-281},
year = {2019},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2019.03.012},
url = {https://www.sciencedirect.com/science/article/pii/S0952197619300624},
author = {Gabriel Trierweiler Ribeiro and Viviana Cocco Mariani and Leandro dos Santos Coelho},
keywords = {Ensembles, Artificial neural network, Wavenet, Load forecasting},
abstract = {Load forecasting implies directly in financial return and information for electrical systems planning. A framework to build wavenet ensemble for short-term load forecasting is proposed in this work. For this purpose, data are first transformed for trend removal and normalization, then an optimal time window is calculated and a subset of features is selected. The bootstrapping, cross-validation like, inputs decimation, constructive selection, simple mean, median, mode and stacked generalization algorithms are used for the ensemble aggregation of wavenet learners. Predictions are realized through one step ahead forecasting strategy. Hourly load values from Italy in 2015 and the GEFCom competition (Global Energy Forecasting Competition) 2012 are used to test and compare the proposed framework with existing similar forecasting techniques such as a multilayer perceptron neural network with sigmoid activation functions in the hidden layer, a single wavenet, a regression tree approach, and the forecasting based on the last week mean. Cross-validated results using 10-folds demonstrate the effectiveness of the proposed forecasting framework based on wavenet ensemble, overcoming performance of the models compared.}
}

@article{fiorucci2020groec,
  title={Groec: combination method via generalized rolling origin evaluation},
  author={Fiorucci, Jose Augusto and Louzada, Francisco},
  journal={International Journal of Forecasting},
  volume={36},
  number={1},
  pages={105--109},
  year={2020},
  publisher={Elsevier}
}

@article{pawlikowski2020,
  title={Weighted ensemble of statistical models},
  author={Pawlikowski, Maciej and Chorowska, Agata},
  journal={International Journal of Forecasting},
  volume={36},
  number={1},
  pages={93--97},
  year={2020},
  publisher={Elsevier}
}


@misc{pavlyshenko2019,
	title={Machine-learning models for sales time series forecasting. Data, 4 (1), 15},
	author={Pavlyshenko, Bohdan M},
	year={2019}
}

@misc{grupo-bimbo-inventory-demand,
    author = {Anna Montoya and Grupo Bimbo and Meghan O'Connell and Wendy Kan},
    title = {Grupo Bimbo Inventory Demand},
    year = {2016},
    howpublished = {\url{https://kaggle.com/competitions/grupo-bimbo-inventory-demand}},
    note = {Kaggle}
}

@article{tonekaboni2020,
  title={What went wrong and when? Instance-wise feature importance for time-series black-box models},
  author={Tonekaboni, Sana and Joshi, Shalmali and Campbell, Kieran and Duvenaud, David K and Goldenberg, Anna},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={799--809},
  year={2020}
}
        
@book{box2015,
  title={Time series analysis: forecasting and control},
  author={Box, George EP and Jenkins, Gwilym M and Reinsel, Gregory C and Ljung, Greta M},
  year={2015},
  publisher={John Wiley \& Sons}
}

@book{zhou2025,
  title={Ensemble methods: foundations and algorithms},
  author={Zhou, Zhi-Hua},
  year={2025},
  publisher={CRC press},
  isbn={9781003587774}
}

@article{clemen1989,
	title={Combining forecasts: A review and annotated bibliography},
	author={Clemen, Robert T},
	journal={International journal of forecasting},
	volume={5},
	number={4},
	pages={559--583},
	year={1989},
	publisher={Elsevier}
}

@book{makridakis_livro_1998,
	title={Forecasting: methods and applications},
	author={Makridakis, Spyros and Wheelwright, Steven and Hyndman, Rob J.},
	year={1998},
	publisher={John Wiley \& Sons}
}

@book{kuncheva2014,
	title={Combining pattern classifiers: methods and algorithms},
	author={Kuncheva, Ludmila I},
	year={2014},
	publisher={John Wiley \& Sons}
}

@book{FPP3,
	title = {Forecasting: Principles and Practice},
	author = {Hyndman, Rob and Athanasopoulos, George},
	edition = {3},
	publisher = {OTexts},
	year = {2021},
	url = {https://otexts.com/fpp3/},
}

@article{ying2015,
  title={Decision tree methods: applications for classification and prediction},
  author={Ying, LU and others},
  journal={Shanghai archives of psychiatry},
  volume={27},
  number={2},
  pages={130},
  year={2015}
}

@book{molnar2025,
	title={Interpretable Machine Learning},
	subtitle={A Guide for Making Black Box Models Explainable},
	author={Christoph Molnar},
	year={2025},
	edition={3},
	isbn={978-3-911578-03-5},
	url={https://christophm.github.io/interpretable-ml-book}
}

@book{pml1Book,
	author = "Kevin P. Murphy",
	title = "Probabilistic Machine Learning: An introduction",
	publisher = "MIT Press",
	year = 2022,
	url = "probml.ai"
}

@article{geurts2006extratrees,
	title={Extremely randomized trees},
	author={Geurts, Pierre and Ernst, Damien and Wehenkel, Louis},
	journal={Machine learning},
	volume={63},
	pages={3--42},
	year={2006},
	publisher={Springer}
}

@book{james2021,
	title={An Introduction to Statistical Learning},
	author={James, Gareth and Witten, Daniela and Hastie, Trevor and Tibshirani, Robert and others},
	subtitle={with Applications in R},
	edition = {2},
	year={2021},
	publisher={Springer},
	doi = {10.1007/978-1-0716-1418-1},
}
@article{ghazwani2023computational,
	title={Computational intelligence modeling of hyoscine drug solubility and solvent density in supercritical processing: Gradient boosting, extra trees, and random forest models},
	author={Ghazwani, Mohammed and Begum, M Yasmin},
	journal={Scientific Reports},
	volume={13},
	number={1},
	pages={10046},
	year={2023},
	publisher={Nature Publishing Group UK London}
}

@article{bergstra2012random,
	title={Random search for hyper-parameter optimization},
	author={Bergstra, James and Bengio, Yoshua},
	journal={The journal of machine learning research},
	volume={13},
	number={1},
	pages={281--305},
	year={2012},
	publisher={JMLR. org}
}

@article{sktime,
	title={sktime: A unified interface for machine learning with time series},
	author={L{\"o}ning, Markus and Bagnall, Anthony and Ganesh, Sajaysurya and Kazakov, Viktor and Lines, Jason and Kir{\'a}ly, Franz J},
	journal={arXiv preprint arXiv:1909.07872},
	year={2019}
}

@article{nixtla,
	title={StatsForecast: Lightning fast forecasting with statistical and econometric models},
	author={Garza, Federico and Canseco, Max Mergenthaler and Chall{\'u}, Cristian and Olivares, Kin G},
	journal={PyCon Salt Lake City, Utah, US},
	volume={2022},
	pages={66},
	year={2022}
}

@article{sklearn,
  title={Scikit-learn: Machine Learning in {P}ython},
  author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
          and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
          and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
          Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
  journal={Journal of Machine Learning Research},
  volume={12},
  pages={2825--2830},
  year={2011}
}

@InProceedings{ pandas,
  author    = { {W}es {M}c{K}inney },
  title     = { {D}ata {S}tructures for {S}tatistical {C}omputing in {P}ython },
  booktitle = { {P}roceedings of the 9th {P}ython in {S}cience {C}onference },
  pages     = { 56 - 61 },
  year      = { 2010 },
  editor    = { {S}t\'efan van der {W}alt and {J}arrod {M}illman },
  doi       = { 10.25080/Majora-92bf1922-00a }
}

@article{autorank,
  title={Autorank: A python package for automated ranking of classifiers},
  author={Herbold, Steffen},
  journal={Journal of Open Source Software},
  volume={5},
  number={48},
  pages={2173},
  year={2020}
}

@Article{matplotlib,
  Author    = {Hunter, J. D.},
  Title     = {Matplotlib: A 2D graphics environment},
  Journal   = {Computing in Science \& Engineering},
  Volume    = {9},
  Number    = {3},
  Pages     = {90--95},
  abstract  = {Matplotlib is a 2D graphics package used for Python for
  application development, interactive scripting, and publication-quality
  image generation across user interfaces and operating systems.},
  publisher = {IEEE COMPUTER SOC},
  doi       = {10.1109/MCSE.2007.55},
  year      = 2007
}



@article{hist_comp,
title = {A brief history of forecasting competitions},
journal = {International Journal of Forecasting},
volume = {36},
number = {1},
pages = {7-14},
year = {2020},
note = {M4 Competition},
issn = {0169-2070},
doi = {https://doi.org/10.1016/j.ijforecast.2019.03.015},
url = {https://www.sciencedirect.com/science/article/pii/S016920701930086X},
author = {Rob J. Hyndman},
keywords = {Evaluation, Forecasting accuracy, Kaggle, M competitions, Neural networks, Prediction intervals, Probability scoring, Time series},
abstract = {Forecasting competitions are now so widespread that it is often forgotten how controversial they were when first held, and how influential they have been over the years. I briefly review the history of forecasting competitions, and discuss what we have learned about their design and implementation, and what they can tell us about forecasting. I also provide a few suggestions for potential future competitions, and for research about forecasting based on competitions.}
}

@article{domingos2012few,
	title={A few useful things to know about machine learning},
	author={Domingos, Pedro},
	journal={Communications of the ACM},
	volume={55},
	number={10},
	pages={78--87},
	year={2012},
	publisher={ACM New York, NY, USA}
}

@article{demsar2006,
	title={Statistical comparisons of classifiers over multiple data sets},
	author={Dem{\v{s}}ar, Janez},
	journal={The Journal of Machine learning research},
	volume={7},
	pages={1--30},
	year={2006},
	publisher={JMLR. org}
}

@article{krogh1994,
  title={Neural network ensembles, cross validation, and active learning},
  author={Krogh, Anders and Vedelsby, Jesper},
  journal={Advances in neural information processing systems},
  volume={7},
  year={1994}
}

@article{lichtendahl2020,
  title={Why do some combinations perform better than others?},
  author={Lichtendahl Jr, Kenneth C and Winkler, Robert L},
  journal={International Journal of Forecasting},
  volume={36},
  number={1},
  pages={142--149},
  year={2020},
  publisher={Elsevier}
}
@article{arian2024,
  title={Stability-Weighted Ensemble Feature Importance for Financial Applications},
  author={Arian, Hamid R and Mousavizade, Seyed Alireza and Seco, Luis A},
  journal={Available at SSRN 4905824},
  year={2024}
}